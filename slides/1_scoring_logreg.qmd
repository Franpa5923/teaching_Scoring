---
title: "Scoring"
subtitle: "M2 D3S/EGR 2025-2026"
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Louis Olive
    email: louis.olive@gmail.com / louis.olive@ut-capitole.fr
date: 2025-09-16
bibliography: references_intro.bib
csl: apa.csl
---

# Outline {background-color="#40666e"}

<!-- Quick and dirty -->
<!-- Lightbox Figures not working with Revealjs / https://quarto.org/docs/output-formats/html-lightbox-figures.html -->
<!-- https://stackoverflow.com/questions/56361986/zoom-function-in-rmarkdown-html-plot/59401761#59401761 -->

<script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js""></script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
</script>

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
```

## Outline

:::{.extrapad}

- (Warm Up) Desbois case study - solutions

- Scoring - reminders

- The Logistic Regression model

:::


# Desbois case study {background-color="#40666e"}

## Desbois case study{.smaller}
### Financial problems of farm holdings

The article from @desbois2008 (available [here](https://csbigs.fr/index.php/csbigs/article/view/351) together with a sample data set) shows a complete case study around the detection of financial risks applicable to farm holdings.

Among others, Linear Discriminant Analysis and Logistic Regression (plus stepwise variable selection procedure) are used. 

ROC curves are then provided to compare the two methods.

The original data set uses a format specific to the SPSS software, but is readable from R using the `foreign` package. 

The case study by Desbois will be partly reproduced today and in the next lessons to quickly apply scoring techniques.

## Desbois case study{.smaller}

Loading Desbois data and first glimpse at it:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

# package used to import spss file
library(foreign)

don_desbois <- read.spss("presentation_data/Agriculture Farm Lending/desbois.sav", to.data.frame = TRUE) %>% as_tibble()
glimpse(don_desbois)
```
## Desbois case study{.smaller}


Replacing for convenience `DIFF` target by factor `Y` with `O` (healthy) or `1` (failing):
```{r}
#| echo: true
#| code-fold: show
#| warning: false

don_desbois <- don_desbois %>%
    mutate(Y = as.factor(if_else(DIFF=='healthy', 0, 1)), DIFF = NULL,
           .before = everything())

```

We give in the following slides the definitions of Desbois ratio $r_1,...,r_{37}$, that we will use today, as found in the article.

They might give you inspiration to create new predictors/features for the project to come.

## Desbois case study{.smaller}
### Capitalization ratios

:::{.extrapad}
- r1 total debt / total assets;
- r2 stockholders' equity / invested capital;
- r3 short term debt / total debt;
- r4 short term debt / total assets;
- r5 long and medium term debt / total assets;
:::

## Desbois case study{.smaller}
### Weight of the debt ratios

:::{.extrapad}
- r6 total debt / gross product;
- r7 long and medium term debt / gross product;
- r8 short term debt / gross product;
:::

## Desbois case study{.smaller}
### Liquidity ratios

:::{.extrapad}
- r11 working capital / gross product;
- r12 working capital / (real inputs - financial expenses);
- r14 short term debt / circulating assets;
:::

## Desbois case study{.smaller}
### Debt servicing ratios

:::{.extrapad}
- r17 financial expenses / total debt;
- r18 financial expenses / gross product;
- r19 (financial expenses + refunding of long and medium term capital) / gross product;
- r21 financial expenses / EBITDA;
- r22 (financial expenses + refunding of long and medium term capital) / EBITDA;
:::

## Desbois case study{.smaller}
### Capital profitability ratio


- r24 EBITDA / total assets;

## Desbois case study{.smaller}
### Earnings ratios

:::{.extrapad}
- r28 EBITDA / gross product;
- r30 available income / gross product;
- r32 (EBITDA - financial expenses) / gross product;
:::

## Desbois case study{.smaller}
### Productive activity ratios

:::{.extrapad}
- r36 immobilized assets / gross product;
- r37 gross product / total assets.
:::

## Desbois case study{.smaller}
### Your turn to play

Using the data set at hand, try to retrieve some findings of the Desbois case study. 

You might need `R` packages `FactoMineR`, `ROCR` and `MASS::lda` function to perform the tasks ("YOUR CODE HERE") described in the following slides. 

Each time I show an example of what is expected.

I suggest that you start using `Quarto` (setup [here](https://quarto.org/docs/get-started/)) to code and track/publish your results. It will be requested for your projects (it is very similar to `RMarkdown`). It integrates perfectly with `RStudio`.


## Desbois case study{.smaller}
### Principal Component Analysis (PCA) of financial ratios

First perform PCA on financial ratios (use for example package [FactoMineR](http://factominer.free.fr/factomethods/principal-components-analysis.html)), then display correlations between ratios and the two first principal components :

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```


```{r}
#| code-fold: true
# More details here http://factominer.free.fr/factomethods/principal-components-analysis.html
res.pca = FactoMineR::PCA(don_desbois,
                          scale.unit = TRUE,
                          quanti.sup = c(4, 7), # HECTARE / AGE excluded from Desbois analysis
                          quali.sup = c(1, 2, 3, 5, 6, 8), 
                          ncp = 5,
                          graph=FALSE)
plot(res.pca, choix = "var")
# FactoMineR::dimdesc(res.pca, axes=c(1,2))
```
## Desbois case study{.smaller}
### Principal Component Analysis (PCA) of financial ratios

```{r}
#| echo: true
#| code-fold: show
#| warning: false

# More details here http://factominer.free.fr/factomethods/principal-components-analysis.html
res.pca = FactoMineR::PCA(don_desbois,
                          scale.unit = TRUE,
                          quanti.sup = c(4, 7), # HECTARE / AGE excluded from Desbois analysis
                          quali.sup = c(1, 2, 3, 5, 6, 8), 
                          ncp = 5,
                          graph=FALSE)
plot(res.pca, choix = "var")
```


## Desbois case study{.smaller}
### To be compared with article Figure 1


![](images/desbois_pca_projection_pc1_pc2.png){.r-stretch}

## Desbois case study{.smaller}

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

Following Desbois path, visualize the farm holdings in data set as a bivariate plot on the first two components of PCA (based on financial ratios), use variable Y (0=”healthy”; 1=”failing”) to colour your observations:

```{r}
#| code-fold: true
# Similar to Desbois Fig 2. 
# Plot of the farm holdings in the first factorial plane of the normalized PCA based on financial ratios
# with illustrative variable Y (0=”healthy”; 1=”failing”)
FactoMineR::plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=1, invisible = c("quali"))
```

## Desbois case study{.smaller}

It strikes Desbois that the PCA (even if not a discriminative/scoring procedure), seems to do a rather good job identifying defaulting farms on the training set.

![](images/desbois_pca_individuals_pc1_pc2.png){.r-stretch}
Disclaimer: in general it won't be necessarily the case as PCA operates only on the predictors without "knowledge" of target variable. 

## Desbois case study{.smaller}

```{r}
#| echo: true
#| code-fold: show
#| warning: false

# Similar to Desbois Fig 2. 
# Plot of the farm holdings in the first factorial plane of the normalized PCA based on financial ratios
# with illustrative variable Y (0=”healthy”; 1=”failing”)
FactoMineR::plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=1, invisible = c("quali"))

```

## Desbois case study{.smaller}

Looking at the preceding plot Desbois concludes that a simple classifier can be devised using the first PCA coordinate (setting a threshold at $PC_1 > 0.02$):

![](images/desbois_pca_rule.png){.r-stretch}


## Desbois case study{.smaller}

As suggested by Desbois, extract the first principal component (some help [here](https://stats.stackexchange.com/questions/460787/pcr-after-pca-with-mixed-data-how-to-extract-export-the-pcs-as-new-variables-i)), and use it as a Scoring function.

Today it will allow us to use a first Scoring function without introducing the Logistic Regression that you have not yet studied in class.

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
# https://stats.stackexchange.com/questions/460787/pcr-after-pca-with-mixed-data-how-to-extract-export-the-pcs-as-new-variables-i
# https://stats.stackexchange.com/questions/494866/how-to-explain-the-numerical-discrepancy-between-factominerpca-and-the-svd
# https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca

# Extracting first five Principal components from FactomineR object 
# Using FactomineR 'notation' (U,sv,V)
# (X nxp matrix of Desbois observations (only r1...r37), SVD decomposition X = U %*% diag(vs) %*% t(V), 
# where U: unitary matrix of left-singular vectors, vs: singular values, V: unitary matrix of left-singular vectors)

# SVD object from FactomineR res.pca$svd

# Principal components X %*% V = U %*% diag(vs) %*% t(V) %*% V = U %*% diag(vs) 
principal_components_1 <- res.pca$svd$U %*% diag(res.pca$svd$vs[1:5])

# Can also be directly extracted from:
principal_components_2 <- res.pca$ind$coord

# Or using PC = X %*% V (where X has been centered and scaled
# base R 'scale' uses a different scaling factor than FactomineR hence sqrt(n / n-1) correction)
X_for_PCA <- don_desbois %>% select(r1:r37) %>% as.matrix()
principal_components_3 <- sqrt(nrow(X_for_PCA) / (nrow(X_for_PCA) - 1)) * scale(X_for_PCA) %*% res.pca$svd$V
```
Below the score for each observation is the x-axis or first principal component:

```{r}
naive_score <-  bind_cols(don_desbois, as_tibble(principal_components_2))
ggplot(naive_score) +
    geom_point(aes(x = Dim.1, y = Dim.2, col = Y)) +
    scale_colour_manual(values = c("dodgerblue", "orange"))
  
```


## Desbois case study{.smaller}
We show below how to obtain the first five Principal components (Desbois only uses the first two):
```{r}
#| echo: true
#| code-fold: show
#| warning: false
# https://stats.stackexchange.com/questions/460787/pcr-after-pca-with-mixed-data-how-to-extract-export-the-pcs-as-new-variables-i
# https://stats.stackexchange.com/questions/494866/how-to-explain-the-numerical-discrepancy-between-factominerpca-and-the-svd
# https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca

# Extracting first five Principal components from FactomineR object 
# Using FactomineR 'notation' (U,sv,V)
# (X nxp matrix of Desbois observations (only r1...r37), SVD decomposition X = U %*% diag(vs) %*% t(V), 
# where U: unitary matrix of left-singular vectors, vs: singular values, V: unitary matrix of left-singular vectors)
# SVD object from FactomineR res.pca$svd

# Principal components X %*% V = U %*% diag(vs) %*% t(V) %*% V = U %*% diag(vs) 
principal_components_1 <- res.pca$svd$U %*% diag(res.pca$svd$vs[1:5])

# Can also be directly extracted from:
principal_components_2 <- res.pca$ind$coord

# Or using PC = X %*% V (where X has been centered and scaled
# base R 'scale' uses a different scaling factor than FactomineR hence sqrt(n / n-1) correction)
X_for_PCA <- don_desbois %>% select(r1:r37) %>% as.matrix()
principal_components_3 <- sqrt(nrow(X_for_PCA) / (nrow(X_for_PCA) - 1)) * scale(X_for_PCA) %*% res.pca$svd$V
# perform svd using base R on scaled matrix
s <- svd(scale(X_for_PCA))
# correct for negative signs
s$v[, c(2, 3, 4, 5)] <- s$v[, c(2, 3, 4, 5)] * -1
V <- s$v[,1:5]
principal_components_4 <- sqrt(nrow(X_for_PCA) / (nrow(X_for_PCA) - 1)) * scale(X_for_PCA) %*% V
```


## Desbois case study{.smaller}
Below we define the score for each observation $x$ as the first principal component $PC_1(x)$ (x-coordinate below, with $PC_2(x)$ as y-coordinate):
```{r}
#| echo: true
#| code-fold: show
#| warning: false
naive_score <-  bind_cols(don_desbois, as_tibble(principal_components_2))
ggplot(naive_score) +
    geom_point(aes(x = Dim.1, y = Dim.2, col = Y)) +
    scale_colour_manual(values = c("dodgerblue", "orange"))
  
```
Defaulting farms or observations $x$ appear in orange, healthy in blue.




## Desbois case study{.smaller}

Going further than Desbois and for illustrative purposes only, use $PC_1$ as a very naive Scoring function. 

Plot below the ROC curve as defined in the Scoring part of the presentation (you can use package `ROCR`, but a simple for-loop for different cutoff values $s$ will do the job):

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
library("ROCR")    

# naive score
pred <- prediction(naive_score$Dim.1, naive_score$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange", 
     print.cutoffs.at = c(-2.5,-1,0.00,1,5), 
     cutoff.label.function = function(x) {paste0("              s = ",round(x,2))})
abline(0, 1) #add a 45 degree line
```
We follow closely the case study, but we will see that ROC curves are usually evaluated on a hold-out data set.

## Desbois case study{.smaller}

```{r}
#| echo: true
#| code-fold: show
#| warning: false

library("ROCR")    

# naive score
pred <- prediction(naive_score$Dim.1, naive_score$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange", 
     print.cutoffs.at = c(-2.5,-1,0.00,1,5), 
     cutoff.label.function = function(x) {paste0("              s = ",round(x,2))})
abline(0, 1) #add a 45 degree line
```


## Desbois case study{.smaller .scrollable}

Then compare with a Scoring function obtained with Linear Discriminant Analysis (LDA). 
For a reminder on LDA see for example @hastie2009[chap. 4.3 LDA, p. 103-111].

You can use Desbois variables selected in the article by a stepwise procedure (using Wilk's lambda[^6]). Selected variables are: 

[^6]:We won’t describe the procedure here. A SPSS script is given in the article. It is not straightforward to reproduce it with R. I tried scripting manually the procedure and it works but is not very interesting. 



![](images/desbois_lda_stepwise.png){.r-stretch}

So basically you can fit LDA with `r2+r3+r7+r14+r17+r18+r21+r32+r36` as predictors (I use `R` formula notation to ease your copy-paste).

## Desbois case study{.smaller}

Fit LDA (`MASS::lda`) with model specification of your choice. Then display model coefficients:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

:::: {.columns}

::: {.column width="60%"}
Using `r2+r3+r7+r14+r17+r18+r21+r32+r36`, we obtain:

```{r}
lda_desbois <- MASS::lda(Y~r2+r3+r7+r14+r17+r18+r21+r32+r36, data=don_desbois)
round(lda_desbois$scaling,3)
```
:::

::: {.column width="40%"}
Similar[^5] to what Desbois obtains:

![](images/desbois_lda_score.png)
:::

::::

[^5]: `R` does not provide [constant](https://stats.stackexchange.com/questions/166942/why-are-discriminant-analysis-results-in-r-lda-and-spss-different-constant-t) for LDA which won't affect Scoring function ordering.


## Desbois case study{.smaller .scrollable}

```{r}
#| echo: true
#| code-fold: show
#| warning: false

lda_desbois <- MASS::lda(Y~r2+r3+r7+r14+r17+r18+r21+r32+r36, data=don_desbois)
round(lda_desbois$scaling,3)
```

## Desbois case study{.smaller .scrollable}

Now compare the LDA Scoring function with the naive Score built with first component of PCA using a ROC curve (on training set):
```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
# for ROCR
labels_desbois <- don_desbois$Y

# naive score
predictions_naive_score <- naive_score$Dim.1

naive_score_group <- naive_score %>%
  group_by(Y) %>%
  summarize(mean_score = mean(Dim.1), n = n())

cutoff_naive <- mean(naive_score_group$mean_score)

pred <- prediction(predictions_naive_score, labels_desbois)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkolivegreen", 
     print.cutoffs.at = c(cutoff_naive), 
     cutoff.label.function = function(x) {paste0("                s = ",round(x,2))})
abline(0, 1) #add a 45 degree line

# lda
predictions_lda <- predict(lda_desbois)$x

lda_score_group <- bind_cols(don_desbois, predictions_lda) %>%
  group_by(Y) %>%
  summarize(mean_score = mean(LD1), n = n())

cutoff_lda <- mean(lda_score_group$mean_score)

pred <- prediction(predictions_lda, labels_desbois)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "plum4",
     print.cutoffs.at = c(cutoff_lda), 
     cutoff.label.function = function(x) {paste0("                s = ",round(x,2))})
legend(0.6,0.6,
       c('naive - PCA', 'LDA'),
       col=c("darkolivegreen", "plum4"),lwd=3)
```

## Desbois case study{.smaller .scrollable}

```{r}
#| echo: true
#| code-fold: show
#| warning: false

# for ROCR
labels_desbois <- don_desbois$Y

# naive score
predictions_naive_score <- naive_score$Dim.1

naive_score_group <- naive_score %>%
  group_by(Y) %>%
  summarize(mean_score = mean(Dim.1), n = n())

cutoff_naive <- mean(naive_score_group$mean_score)

pred <- prediction(predictions_naive_score, labels_desbois)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkolivegreen", 
     print.cutoffs.at = c(cutoff_naive), 
     cutoff.label.function = function(x) {paste0("                s = ",round(x,2))})
abline(0, 1) #add a 45 degree line

# lda
predictions_lda <- predict(lda_desbois)$x

lda_score_group <- bind_cols(don_desbois, predictions_lda) %>%
  group_by(Y) %>%
  summarize(mean_score = mean(LD1), n = n())

cutoff_lda <- mean(lda_score_group$mean_score)

pred <- prediction(predictions_lda, labels_desbois)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "plum4",
     print.cutoffs.at = c(cutoff_lda), 
     cutoff.label.function = function(x) {paste0("                s = ",round(x,2))})
legend(0.6,0.6,
       c('naive - PCA', 'LDA'),
       col=c("darkolivegreen", "plum4"),lwd=3)
```

## Desbois case study{.smaller .scrollable}

As a very soft and intuitive introduction to logistic regression. First discretize a ratio $r$ of your choice.

:::: {.columns}

::: {.column width="50%"}
- Compute proportion of defaults among each class. 
For example using $0.01$ steps for ratio $r17$:
```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r, warning = FALSE, message = FALSE}
#| code-fold: true
class_width <- 0.01
don_desbois_binned <- don_desbois %>%
    mutate(r17_bins = cut(r17, breaks = seq(0, 0.2, class_width),
                              right = FALSE, dig.lab = 4, include.lowest = TRUE),
           min = floor(r17 / class_width) * class_width,
           max = if_else(r17 == 0 , 1, 
                         # customers with 0$ balance should belong to [0, width) class
                         # or be excluded
                         ceiling(r17 / class_width))  * class_width,
           max = if_else(min==max, (ceiling(r17 / class_width) + 1)  * class_width, max)) %>% 
    group_by(r17_bins, min, max, Y) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = Y, values_from = n) %>%
    replace_na(list(`0` = 0, `1` = 0)) %>% 
    mutate(`Mean(Y)` = round(`1` / (`1` + `0`), 4))
cat(simplermarkdown::md_table(don_desbois_binned %>% select(r17_class=r17_bins, `0`, `1`,`Mean(Y)` )))
```

:::

::: {.column width="50%"}

- Then using this table, plot an empirical conditional distribution of default given $r$ classes: 

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
#| code-fold: true
(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25)) +
 scale_y_continuous(breaks = c(0, 1))
```
:::
::::

## Desbois case study{.smaller .scrollable}

Compute proportion of defaults among each class. For example using $0.01$ steps for ratio $r17$:
```{r}
#| echo: true
#| code-fold: show
#| warning: false
class_width <- 0.01
don_desbois_binned <- don_desbois %>%
    mutate(r17_bins = cut(r17, breaks = seq(0, 0.2, class_width),
                              right = FALSE, dig.lab = 4, include.lowest = TRUE),
           min = floor(r17 / class_width) * class_width,
           max = if_else(r17 == 0 , 1, 
                         # customers with 0$ balance should belong to [0, width) class
                         # or be excluded
                         ceiling(r17 / class_width))  * class_width,
           max = if_else(min==max, (ceiling(r17 / class_width) + 1)  * class_width, max)) %>% 
    group_by(r17_bins, min, max, Y) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = Y, values_from = n) %>%
    replace_na(list(`0` = 0, `1` = 0)) %>% 
    mutate(`Mean(Y)` = round(`1` / (`1` + `0`), 4))
cat(simplermarkdown::md_table(don_desbois_binned %>% select(r17_class=r17_bins, `0`, `1`,`Mean(Y)` )))
```



## Desbois case study{.smaller .scrollable}
Then using this table, plot an empirical conditional distribution of default given $r$ classes: 

```{r}
#| echo: true
#| code-fold: show
#| warning: false

(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25)) +
 scale_y_continuous(breaks = c(0, 1))
```



## Desbois case study{.smaller .scrollable}

```{r}
#| code-fold: true
(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_smooth(method = "glm", 
             formula = y ~ x,
             method.args = list(family = "binomial"), 
             se = FALSE,
             col = "dodgerblue",
             linetype = "dotted") +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25)) +
 scale_y_continuous(breaks = c(0, 1))
```
We notice that the mean default occurrence with respect to $r_{17}$ classes follows a kind of "S"-shaped curve or **sigmoid** function.
Note that the shape depends on classes width and might change.
We "fitted" (blue-dotted) $\sigma: x \to\sigma(x)=\frac{e^{x}} { 1 + e^{x} }$ also known as logistic function to the mean default occurences (orange segments).


## Desbois case study{.smaller}
This roughly corresponds to the intuitive introduction to the logistic regression model given in @hosmer2013 using a Coronary Heart Disease (CHD) event as $Y$ and AGE as $X$:

:::: {.columns}

::: {.column width="50%"}

![](images/hosmer_lemeshow_chd3.png)
:::

::: {.column width="50%"}

![](images/hosmer_lemeshow_chd4.png)
:::

::::

## Desbois case study{.smaller .scrollable}

```{r}
#| echo: true
#| code-fold: show
#| warning: false
(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_smooth(method = "glm", 
             formula = y ~ x,
             method.args = list(family = "binomial"), 
             se = FALSE,
             col = "dodgerblue",
             linetype = "dotted") +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25)) +
 scale_y_continuous(breaks = c(0, 1))
```

# Scoring - reminders {background-color="#40666e"}



## Scoring{.smaller}
### Supervised learning - Loss

To achieve that goal we try to find a "best" or optimal **classifier** (or prediction function in general):

$$
f: \mathrm X \to \mathrm Y
$$

We first must define some criterion to assess the classifier performance (what is optimal?).

For that purpose we consider a **loss** function $\ell: \mathrm Y \times \mathrm Y \to \mathbb R^+$:

$$
\left\{ \begin{array}{ll} 
    \ell(y,z) > 0 &  \mbox{if } y \neq z\cr
    \ell(y,z) = 0 &  \mbox{if } y = z\cr
\end{array} \right.
$$

$\ell(y,f(x))$ is the loss or cost of predicting $f(x)$ while the true output is $y$.

In the context of binary classification a natural selection for loss is the 0-1 loss:

$$
\ell(y,z)=\mathbf 1_{y \neq z}
$$

We will see later in the course that other losses are used.

## Scoring{.smaller}
### Supervised learning - Risk

We then define the expected **risk** of a prediction function $f$ given the loss $\ell$ as:

$$
\mathrm R(f) = \mathbb{E}[\ell(Y,f(X))]
$$ In the context of binary classification $Y \in \{0,1\}$ and 0-1 loss, we have:

$$
\mathrm R(f) = \mathbb{P}[Y\neq f(X))]
$$

Given a data set $(x_i, y_i) \in \mathrm X \times \mathrm Y$, we also define the empirical risk of a classifier $f$ as:

$$
\hat{\mathrm R}(f) = \frac{1}{n} \sum_i \ell(y_i,f(x_i))
$$

## Scoring{.smaller .scrollable}
### Supervised learning - Bayes classifier

We now define the **Bayes(ian) classifier**[^3] $f^*: \mathrm X \to \mathrm Y$ as a function that achieves the minimal expected risk among all possible functions:

$$
\underset{f}{\operatorname{argmin}}\mathrm R(f) 
$$

[^3]: It is sometimes designed as oracle in the literature.

The Bayes classifier depends on $\ell$ and $\mathbf P_{\mathrm X \times \mathrm Y}$ which is generally unknown (otherwise the job would be easy).

The Bayes classifier for the 0-1 loss is:

$$
f^*(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } \eta(x)\geq \frac{1}{2}\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

where:

$$
\eta(x)=\mathbb{P}[Y=1|X=x)] = \mathbb{E}[Y=1|X=x)]
$$

$\eta(x)$ is called the regression function.

## Scoring{.smaller .scrollable}
### Supervised learning - Theory vs practice

In practice $\mathbf P_{\mathrm X \times \mathrm Y}$ is unknown. What we have is the learning set. The Bayes classifier minimizes the expected risk but is not a function of the learning set. What to do? Conceptually two approaches are used[^4]:

[^4]: See Sébastien Gadat - TSE - Maths of Deep and Machine Learning course [here](https://perso.math.univ-toulouse.fr/gadat/files/2012/12/Note-25-oct.-2022.pdf). Other very good references are Philippe Rigollet - MIT - 18.657: Mathematics of Machine Learning [here](https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/81406c87dccb9e873cfafa876a4d69c3_MIT18_657F15_LecNote.pdf) or Erwan Le Pennec - Polytechnique - Introduction to Machine Learning (M2 MSV) [slides here](http://www.cmap.polytechnique.fr/~lepennec/files/MSV/MLMethods_Adv%20-%20MSV%20-%2023.pdf). We will see in another course that in the context of the Logistic Regression the two approaches are strongly linked: it can be seen as a probabilistic\|discriminative approach or an optimization approach.

-   the "probabilistic" approach: we estimate the regression function $\eta(x)$ and use/plug this estimate "inside" the Bayes classifier (Plugin); within this approach mainly two sub-approaches:

    - the "discriminative" approach: we directly model or make an assumption on $\eta(X)=\mathbf P_{\mathrm Y | \mathrm X}$ and estimate it; for example we can make the assumption that $\eta(x)$ is a function of a particular form.

    - the "generative" approach: we model the conditional distribution $\mathbf P_{\mathrm X | \mathrm Y}$ and use Bayes formula to get: $\eta(X)=\frac{\mathbb{P}[X|Y=1)]\mathbb{P}[Y=1)]}{\mathbb{P}[X]}=\frac{\mathbb{P}[X|Y=1)]\mathbb{P}[Y=1)]}{\mathbb{P}[X|Y=1)]\mathbb{P}[Y=1)]+\mathbb{P}[X|Y=0)]\mathbb{P}[Y=0)]}$

-   the optimization or "machine learning" approach: finding $f$ minimizing the empirical risk; optimization algorithm or heuristics are used to estimate $f$; the 0-1 loss is not convex so usually it is replaced by a convex surrogate loss or upper bound; usually re-sampling methods are used to compute empirical risk ($f$ is trained on a training set and empirical risk evaluated on the testing set, $f$ is chosen to minimize empirical risk).


## Scoring{.smaller}
### Scoring function

As introduced in @cornillon2019 [chapter 11.6 "Prévision - scoring"]:

- The aim of Scoring: find a Scoring function or Score $S: \mathbb R^p \to \mathbb R$ that is "high" in case $\mathbb{P}[Y=1|X=x)$ is "high" and "low" in case $\mathbb{P}[Y=0|X=x)$ is "high".

- We say that $S(x)$ is the score of the observation $x \in \mathbb R^p$.

- The notions of Score and classifier are closely linked. Given a Score $S$ and a cutoff $s \in \mathbb R$ we obtain the following classifier:

$$
f_s(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } S(x)\geq s\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

-   The values taken by $S(x)$ are less important than the way they will order a set of observation $x_1,...x_n$. For any $\phi: \mathbb R \to \mathbb R$ bijective and increasing function, we say that the scores $S$ and $\phi \circ S$ are equivalent.

## Scoring{.smaller}
### First example

We show below a simplified example of a credit scorecard that were typically used by banks to assess the creditworthiness of consumer loans applicants (as shown in [@scoringThomas]):

![](images/scoring_scorecard.png){.r-stretch}

## Scoring{.smaller}
### First example

![](images/scoring_scorecard.png){.r-stretch}

- a 35-year-old owner wishing to borrow money for home improvement and that has never had a county court judgement (CCJ) will score $129 = (36+25+36+32)$,

- a 20-year-old living with his parents borrowing for holiday and having more than £1200 of CCJ will score $38=(22+14+19-17)$.

## Scoring{.smaller}
### The regression function, a Scoring function

We defined above, in the context of supervised learning, the regression function $\eta(x)=\mathbb{P}[Y=1|X=x)]$. 

It is a natural candidate for a Scoring function. 

The related Bayes classifier uses $s=\frac{1}{2}$ as cutoff. 

As we have just seen in the last sections, $\eta(x)$ is unknown and we will have to approximate or estimate this regression function using the training set.

## Scoring{.smaller}
### Evaluating a Score

We first define the **confusion matrix**:

|       | $f_s(X)=0$ | $f_s(X)=1$ |     |
|:------|:----------:|:----------:|:---:|
| $Y=0$ |     TN     |     FP     |  N  |
| $Y=1$ |     FN     |     TP     |  P  |

: {.bordered}

The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is a contingency table which cross-tabulates $Y$ with the predicted one $f_s(X)$.

It can be evaluated on the learning set as well as on the testing set.

The following vocabulary originates from medical applications:

-   true positive (TP)

-   true negative (TN),

-   false positive (FP), also Type I error

-   false negative (FN), also Type II error

## Scoring{.smaller}
### Evaluating a Score

More formally we define for a given cutoff $s$:

$$
\alpha(s)=\mathbf P(f_s(X)=1|Y=0)=P(S(X) \geq s|Y=0)
$$

and

$$
\beta(s)=\mathbf P(f_s(X)=0|Y=1)=P(S(X) < s|Y=1)
$$

$\alpha(s)$ is called **false positive** rate and $\beta(s)$ **false negative** rate. Similarly **specificity** and **sensitivity** are defined:

$$
sp(s)=P(S(X) < s|Y=0) = 1 - \alpha(s)
$$

and

$$
se(s)=P(S(X) \geq s|Y=1) = 1 - \beta(s)
$$

## Scoring{.smaller}
### The Receiver Operating Characteristic (ROC) curve

The ROC (Receiver Operating Characteristic) curve allows to visualize $\alpha$ and $\beta$ on a same graph for all $s$ allowing to choose the cutoff and compare different Scores.

Precisely, the ROC curve of a Score $S$ is a parametric curve of the variable $s$:

$$
\begin{array}{ll} 
    ROC:&\mathbb R \to [0,1]^2 \cr
        & s \to (x(s)=\alpha(s),y(s)=1-\beta(s))\cr
\end{array}
$$

For a given Score $S$, the ROC curve passes through the points $(0,0)$ and $(1,1)$, which corresponds to classifying all observations as $0$ ($s\to \infty$) or $1$ ($s\to -\infty$).


## Scoring{.smaller}
### The Receiver Operating Characteristic (ROC) curve

@fig-roc-perfect-random shows the two curves.

```{r, warning=FALSE}
#| label: fig-roc-perfect-random
#| fig-cap: "ROC curves of a perfect Score (solid, blue) and a random Score (dashed, orange)"


perfect_score_label = "s=s^{\\*}"
temp <- expression("s="~rho == 0.34)

ggplot() +
    geom_segment(aes(x = 0, y = 0, xend = 0, yend = 1), col="dodgerblue", linewidth = 2) + # perfect score vert axis
    geom_segment(aes(x = 0, y = 1, xend = 1, yend = 1), col="dodgerblue", linewidth = 2) + # perfect score horiz axis
    geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), col="orange", linetype = 2, linewidth = 1.5) + # random score
    # annotate perfect score
    annotate("segment", x = 0.05, y = 0.95, xend = 0, yend = 1,
             arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("text", x=0.08,y=0.92, label = 's == "s*"', parse=TRUE) +
    # -Inf
    annotate("segment", x = 0.95, y = 0.95, xend = 1, yend = 1,
             arrow = arrow(type = "closed", length = unit(0.02, "npc")))+
    annotate("text", x=0.92,y=0.92, label = expression(s==-infinity), parse=TRUE) +
    # +Inf
    annotate("segment", x = 0.05, y = 0.05, xend = 0, yend = 0,
             arrow = arrow(type = "closed", length = unit(0.02, "npc")))+
    annotate("text", x=0.08,y=0.08, label = expression(s==infinity), parse=TRUE) +
    coord_cartesian(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE) +
    labs(x = "false positives", y = "true positives") +
    theme_bw() 

```

# The Logistic Regression model {background-color="#40666e"}

## The Logistic Regression model{.smaller}
### Informal introductory example
We provide here some intuitions leading to the Logistic Regression model using a simulated data set from @islr2021 (the **Default** data set).

```{r}
#| echo: true
#| code-fold: show
#| warning: false
# Default data set (simulated) from ESLII/ISLR
default_data <- ISLR2::Default %>%
    as_tibble()

glimpse(default_data)
```
This is a toy data set used for teaching purposes containing information on ten thousand customers.

The aim here is to assess which customers will ***default*** on their credit card debt (the target or response variable) based on the current credit card ***balance*** and other individual characteristics (the predictors or feature vector).

## The Logistic Regression model{.smaller}
### Informal introductory example
We can start to explore the Default data with a scatterplot (@fig-default_balance-scatterplot) of the target variable (***default***) with respect to a predictor (***balance***):
```{r}
#| label: fig-default_balance-scatterplot
#| fig-cap: "Scatterplot of variable ***default*** with respect to credit card ***balance*** for 10000 customers"

ggplot(default_data, aes(x=balance, y=default)) +
geom_point(alpha=0.2)
```

## The Logistic Regression model{.smaller}
### Informal introductory example

In this scatterplot, all points fall on one of two parallel lines representing the absence (No) or occurrence (Yes) of ***default***. We "jitter" the data vertically to avoid overplotting. The plot below shows that the response variable is imbalanced towards the absence of default:

```{r}
ggplot(default_data, aes(x=balance, y=default)) +
geom_jitter(alpha=0.2, height=0.2)
```
## The Logistic Regression model{.smaller}
### Informal introductory example

We also show the boxplots of credit cards ***balance*** with respect to ***default*** status:

```{r}
#| label: fig-balance_default-boxplot
#| fig-cap: "Variable ***balance*** with respect to ***default*** status"
ggplot(default_data,
       aes(x=default, y=balance)) +
geom_boxplot()
```

We can see from @fig-default_balance-scatterplot and @fig-balance_default-boxplot that default tends to be more prevalent for accounts with a high balance. However it is difficult to guess a simple relationship between default and balance.

## The Logistic Regression model{.smaller}
### Informal introductory example

To investigate further we discretise the balance variables by classes of width $300\$$ and compute the mean of response variable (***default*** is Yes) within each balance class:

```{r, message = FALSE}
class_width <- 300
(default_data_binned <- default_data %>%
    mutate(balance_bins = cut(balance, breaks = seq(0, 3000, class_width),
                              right = FALSE, dig.lab = 4),
           min = floor(balance / class_width) * class_width,
           max = if_else(balance == 0 , 1, 
                         # customers with 0$ balance should be long to [0, width) class
                         # or be excluded
                         ceiling(balance / class_width))  * class_width) %>% 
    group_by(balance_bins, min, max, default) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = default, values_from = n) %>%
    replace_na(list(Yes = 0, No = 0)) %>% 
    mutate(`Mean(default)` = round(Yes / (Yes + No), 4)))
```


## The Logistic Regression model{.smaller}
### Informal introductory example

Then we plot the mean of default (in red) within each balance class (of width $300\$$):

```{r}
#| label: fig-balance_default-scatterplot-occurrence
#| fig-cap: "Mean occurrence of ***default*** within ***balance*** classes"

(default_occurrence <- 
 ggplot(default_data %>% mutate(default = if_else(default == "Yes", 1, 0)), aes(x=balance, y=default)) +
 geom_point(alpha=0.2) +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              linewidth=1.25))
```

The relationship between the mean occurrence of ***default*** and ***balance*** is easier to read.

@fig-balance_default-scatterplot-occurrence clearly shows that as balance increases, the proportion of customers defaulting on their credit card increases.

## The Logistic Regression model{.smaller}
### Informal introductory example

```{r}
(default_occurrence)
```
We also notice that the mean default occurrence with respect to balance classes follows a kind of "S"-shaped curve or **sigmoid** function. 
Going further and informally, considering that the mean of default occurrence is an estimate of $\mathbf{E}[Y|X=x]$ for each balance classes an idea would be to model:

$$
 \mathbb{E}[Y|X=x] = \mu_\beta(x)
$$

where $\mu_\beta$ is a **sigmoid** function in $[0,1]$.

## The Logistic Regression model{.smaller}
### Informal introductory example

The Logistic Regression model uses the **sigmoid** function $\sigma: x \to\sigma(x)=\frac{e^{x}} { 1 + e^{x} }$ also known as the logistic function. Below a simple transform of this logistic function has been "fitted" (blue dots) to the `default ~ balance` data set:

```{r}
(default_occurrence +
 geom_smooth(method = "glm", 
             formula = y ~ x,
             method.args = list(family = "binomial"), 
             se = FALSE,
             col = "dodgerblue",
             linetype = "dotted") +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              linewidth=1.25))
```

## The Logistic Regression model{.smaller}
### A more formal definition - the discriminative approach

Reminding the statistical learning / scoring concepts introduced before. We try to predict the output `default` ($Y \in \{0,1\}$) using a training set of inputs $X$: this is a binary classification problem.

We remind that to estimate an optimal classifier for output $Y \in \{0,1\}$ using input $X = (X_1,\cdots,X_p)$ one approach was to:

-   model the conditional distribution $Y|X$ (the **discriminative** approach),

-   estimate $\eta(x)$ with:

$$
\eta(x)=\mathbb{P}[Y=1|X=x)] = \mathbb{E}[Y=1|X=x)]
$$

-   $\eta(x)$ can be used as a Scoring function

-   define the classifier $f_s$ using a cutoff $s$ and Scoring function $\eta(x)$ to predict output $Y$:

$$
f_s(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } \eta(x)\geq s\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

## The Logistic Regression model{.smaller}
### A more formal definition

In the case of **Logistic Regression** classifier, we model:

$$
Y|X=x~ \sim B(\eta(x))
$$

with

$$
\eta(x)=\sigma(x^T\beta) =\frac{exp(x^T\beta)}{1+exp(x^T\beta)}
$$

for some parameter $\beta=(\beta_1,\cdots,\beta_p)\in \mathbb R^p$, usually $x_1=1$ and $\beta_1$ is an intercept. $\sigma$ is the sigmoid logistic function we have seen before.

In the literature is usual to denote $\eta(X)=p_{\beta}(X)$ or $\eta(X)=\pi_{\beta}(X)$.

From now, we will use the notation $p_{\beta}(X)$.

## The Logistic Regression model{.smaller}
### A more formal definition

Defining $\mathrm{logit}: x \to \log\bigg( \frac{x}{1-x}\bigg)$, which is the inverse of $\sigma$ the logistic function (show it as an exercise), we have:

$$
\mathrm{logit}(p_{\beta}(X))=X^T\beta
$$

::: {.callout-tip icon="false"}
## The **Logistic Regression** model:

We are given $(x_i, y_i) \in \mathbb R^p \times \{0,1\}$, $i=1,\cdots,n$

The Logistic Regression model assumes that outputs $y_i$ are independent Bernoulli with parameter $p_{\beta}(x_i)$ depending on $x_i$:

$$
\mathrm{logit}(p_{\beta}(x_i))=x_i^T\beta
$$
:::

## The Logistic Regression model{.smaller}
### How to fit with R

The syntax to fit the Logistic model in R using `glm()` is:

$$
\mathtt{glm(} \mathrm{y} \sim \mathrm{~x,~}\mathtt{data=}~\mathrm{dataframe,~}\mathtt{family~=~binomial(link~=~"logit")}
$$

The formula $\mathrm{y} \sim \mathrm{x}$ depicts the model (i.e. inputs are $X$, output is $Y$) and the `data=` argument points to the training set contained in a R dataframe (or tibble). This is quite similar to the `lm()` function.

We also need to specify the distribution for the conditional $Y$ values (binomial) and the link function (logit) via the `family=` argument.

## The Logistic Regression model{.smaller}
### How to fit with R
For our default example:

```{r}
#| code-fold: show
glm_default <- glm(default ~ 1 + student + balance + income,
                   data = default_data,
                   family = binomial(link = "logit"))

glm_default <- glm(default ~ .,
                   data = default_data,
                   family = "binomial") # by default: link = "logit"
 
```

The command `summary` produces result summaries of the fitted model:

```{r}
#| code-fold: show
summary(glm_default)
```

We will see in the next lesson, how this model is fitted in practice and how to interpret or understand what is printed by the `summary` function.

## References
::: {#refs}
:::
